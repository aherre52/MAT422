{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNrQw1ug9h6c9MEfdV3FEEQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aherre52/MAT422/blob/main/HW_3_7_MAT_422.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **HW 3.7: Neural Networks**\n",
        "\n",
        "Concepts covered:\n",
        "\n",
        "\n",
        "* 3.7.1. Mathematical formulation\n",
        "* 3.7.2. Activation functions\n",
        "* 3.7.3. Cost function\n",
        "* 3.7.4. Backpropagation\n",
        "* 3.7.5. Backpropagation algorithm"
      ],
      "metadata": {
        "id": "ix_h5JSajFrt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.7.1. Mathematical formulation\n",
        "\n",
        "Artificial neural networks (ANNs) are computational models inspired by the structure of biological neural networks, with layers of interconnected nodes (neurons) that learn from data to perform tasks such as classification or regression. In a basic neural network, inputs $x_1$ and $x_2$ pass through nodes with weights $w_1$ and $w_2$ and a bias $b$, yielding an output $\\hat{y}$ through an activation function $\\sigma(z)$. The network adjusts its weights and biases using a learning algorithm that minimizes a cost function, gradually improving the output's accuracy with respect to the target values. For a neural network with multiple layers, each node in layer $l$ is calculated by combining values from the previous layer $l-1$ using weights and biases, forming expressions like $z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}$, where $W^{(l)}$ is the weight matrix and $b^{(l)}$ is the bias vector. The activation function $\\sigma(z)$ is then applied, resulting in the node values $a^{(l)}$ for layer $l$, which allows for complex, non-linear mappings from inputs to outputs in the network."
      ],
      "metadata": {
        "id": "leV5PiXPKv44"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code description\n",
        "\n",
        "This code calculates the output of a simple neural network layer with two inputs, weights, a bias, and an activation function. It computes the linear combination $$ z = w_1 \\cdot x_1 + w_2 \\cdot x_2 + b $$ and then applies the sigmoid activation function $$ \\hat{y} = \\frac{1}{1 + e^{-z}} $$ to produce the final output. The result, $ \\hat{y} $, represents the activated output of the layer given the specified parameters."
      ],
      "metadata": {
        "id": "rmsiSoZ59bwr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the input values arbitrarily, just two\n",
        "x1 = 1.5\n",
        "x2 = 2.0\n",
        "\n",
        "# Define the weights and bias arbitrarily\n",
        "w1 = 0.6  # Give weight for x1\n",
        "w2 = 0.4  # Give weight for x2\n",
        "b = 0.2   # the bias term\n",
        "\n",
        "# Define the activation function (will use sigmoid in this example)\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# WIth everything set up, calculate the linear combination of\n",
        "# inputs and weights plus bias\n",
        "z = w1 * x1 + w2 * x2 + b\n",
        "\n",
        "# Now apply the activation function using sigmoid\n",
        "y_hat = sigmoid(z)\n",
        "\n",
        "print(\"The output of the neural network layer is:\", y_hat)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRWTzJW_oNYj",
        "outputId": "83e34a3b-0ccd-44e2-9e77-2b92b5086451"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The output of the neural network layer is: 0.8698915256370021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.7.2. Activation functions and 3.7.3. Cost Function\n",
        "\n",
        "In neural networks, **activation functions** play a key role by determining the output of each neuron in response to inputs, often tailored to tasks like classification. Activation functions simulate the \"firing\" mechanism in biological neurons and are denoted by $\\sigma$. For a given layer, the activation output is represented by\n",
        "\n",
        "$$ a^{(l)} = \\sigma \\left( z^{(l)} \\right) = \\sigma \\left( W^{(l)} a^{(l-1)} + b^{(l)} \\right) . $$\n",
        "\n",
        "Common activation functions include:\n",
        "\n",
        "* **step function** $ \\sigma(x) = \\begin{cases} 0, & x < 0 \\\\ 1, & x \\geq 0 \\end{cases} $, which is binary and useful for classification tasks.\n",
        "* **ReLU function** $ \\sigma(x) = \\max(0, x) $ is widely used in deep networks for its efficiency, as it either allows signals to pass or blocks them entirely.\n",
        "* **sigmoid function** $ \\sigma(x) = \\frac{1}{1 + e^{-x}} $ outputs values between 0 and 1, often used for probabilistic interpretations\n",
        "* **softmax function** transforms a vector into probabilities across multiple classes:\n",
        "\n",
        "$$ \\sigma(z)_k = \\frac{e^{z_k}}{\\sum_{j=1}^K e^{z_j}} . $$\n",
        "\n",
        "To train the network, a **cost function** $ J $ is used, such as least squares for regression:\n",
        "\n",
        "$$ J = \\frac{1}{2} \\sum_{n=1}^N \\sum_{k=1}^K \\left( \\hat{y}_k^{(n)} - y_k^{(n)} \\right)^2 , $$\n",
        "\n",
        "or cross-entropy for binary classification:\n",
        "\n",
        "$$ J = - \\sum_{n=1}^N \\left( y^{(n)} \\ln \\hat{y}^{(n)} + (1 - y^{(n)}) \\ln (1 - \\hat{y}^{(n)}) \\right) . $$\n",
        "\n",
        "These functions drive the learning process by optimizing network parameters through methods like gradient descent, enabling the model to accurately capture patterns in the data."
      ],
      "metadata": {
        "id": "XyE9tF1ZLimK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code description\n",
        "\n",
        "This code demonstrates the application of common activation functions used in neural networks, including the step function, ReLU, sigmoid, and softmax, to a sample input array. Each activation function transforms the input values in a distinct way: the step function outputs binary values, ReLU blocks negative values, sigmoid squashes the input between 0 and 1, and softmax converts a vector of values into probabilities.\n",
        "\n",
        "Additionally, the code calculates the least squares cost function, which measures the difference between predicted and true values, commonly used in regression tasks. This helps visualize both the functionality of activation functions and the evaluation of model performance using a cost function."
      ],
      "metadata": {
        "id": "vVzSiubmxLzp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Activation Functions\n",
        "def step_function(x):\n",
        "    return np.where(x >= 0, 1, 0)\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x))  # for numerical stability\n",
        "    return exp_x / np.sum(exp_x, axis=0)\n",
        "\n",
        "# Sample input\n",
        "x = np.array([-1.0, 0.0, 1.0])\n",
        "\n",
        "# Apply activation functions\n",
        "print(\"Step Function:\", step_function(x))\n",
        "print(\"ReLU:\", relu(x))\n",
        "print(\"Sigmoid:\", sigmoid(x))\n",
        "print(\"Softmax:\", softmax(x))\n",
        "\n",
        "# Cost Function - Least Squares\n",
        "def least_squares_cost(y_true, y_pred):\n",
        "    return 0.5 * np.sum((y_pred - y_true) ** 2)\n",
        "\n",
        "# Sample true and predicted outputs\n",
        "y_true = np.array([0.5, 1.0, 0.5])\n",
        "y_pred = np.array([0.4, 1.2, 0.3])\n",
        "\n",
        "# Calculate cost\n",
        "print(\"Least Squares Cost:\", least_squares_cost(y_true, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEi4LwTBtCce",
        "outputId": "c2462c29-9804-44a2-e291-ba7ab2d43517"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step Function: [0 1 1]\n",
            "ReLU: [0. 0. 1.]\n",
            "Sigmoid: [0.26894142 0.5        0.73105858]\n",
            "Softmax: [0.09003057 0.24472847 0.66524096]\n",
            "Least Squares Cost: 0.04499999999999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.7.4. Backpropagation and 3.7.5. Backpropagation Algorithm\n",
        "\n",
        "Backpropagation is a key process in neural network training, aimed at minimizing the cost function $J$ with respect to the network parameters, such as weights $W$ and biases $b$. To achieve this, the derivatives of $J$ with respect to these parameters are computed using the chain rule. The quantity $\\delta^{(l)}_j = \\frac{\\partial J}{\\partial z^{(l)}_j}$ is introduced, which represents the error at each node in layer $l$. This error is propagated backward through the network, starting from the output layer, and is used to update the weights and biases. The updates are done using gradient descent, where the new weights and biases are calculated as follows:\n",
        "\n",
        "$$\n",
        "\\text{New } w^{(l)}_{j,j} = \\text{Old } w^{(l)}_{j,j} - \\beta \\delta^{(l)}_j a^{(l-1)}_j\n",
        "$$\n",
        "$$\n",
        "\\text{New } b^{(l)}_j = \\text{Old } b^{(l)}_j - \\beta \\delta^{(l)}_j\n",
        "$$\n",
        "\n",
        "This process is repeated iteratively, adjusting the weights and biases until the desired accuracy is achieved. The derivatives depend on the chosen activation function, such as ReLU or logistic, which affects the backpropagation process.\n"
      ],
      "metadata": {
        "id": "amrwv-knjBS-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The provided code implements a simple neural network with one input layer and one output layer, using the sigmoid activation function. During training, the network performs a forward pass where it calculates the predicted output using the input data and weights. The Mean Squared Error (MSE) cost function is used to measure the difference between the predicted output and the target. Backpropagation is then used to compute the gradients of the cost function with respect to the weights and biases, updating the parameters using gradient descent to minimize the cost. The process is repeated over multiple passes until the model converges to a solution.\n"
      ],
      "metadata": {
        "id": "Zq6JtwpmQ8H7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sigmoid activation function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Derivative of sigmoid\n",
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "# Mean Squared Error cost function\n",
        "def mse_cost(y_pred, y_true):\n",
        "    return np.mean((y_pred - y_true) ** 2)\n",
        "\n",
        "# Derivative of MSE cost function\n",
        "def mse_cost_derivative(y_pred, y_true):\n",
        "    return 2 * (y_pred - y_true) / y_true.size\n",
        "\n",
        "# Forward pass function for a simple neural network with one layer\n",
        "def forward_pass(X, W1, b1):\n",
        "    z1 = np.dot(X, W1) + b1\n",
        "    a1 = sigmoid(z1)  # activate output layer\n",
        "    return z1, a1\n",
        "\n",
        "# Backpropagation for the network\n",
        "def backpropagation(X, y, W1, b1, z1, a1, learning_rate=0.1):\n",
        "    # Compute the error at the output layer\n",
        "    d_a1 = mse_cost_derivative(a1, y) * sigmoid_derivative(z1)\n",
        "\n",
        "    # Compute the gradients for weights and biases\n",
        "    d_W1 = np.dot(X.T, d_a1)\n",
        "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
        "\n",
        "    # Update weights and biases using gradient descent\n",
        "    W1 -= learning_rate * d_W1\n",
        "    b1 -= learning_rate * d_b1\n",
        "\n",
        "    # Return updated parameters\n",
        "    return W1, b1\n",
        "\n",
        "# Initialize random weights and biases for a 1-layer neural network\n",
        "np.random.seed(42)\n",
        "input_size = 3  # Input size /features\n",
        "output_size = 1  # Size of output layer\n",
        "\n",
        "W1 = np.random.randn(input_size, output_size)\n",
        "b1 = np.zeros((1, output_size))\n",
        "\n",
        "# Input data (2 samples and 3 features)\n",
        "X = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]])\n",
        "\n",
        "# Target output (2 samples and a target)\n",
        "y = np.array([[0.7], [0.9]])\n",
        "\n",
        "# Training loop (100 iterations)\n",
        "for passes in range(100):\n",
        "    # Perform forward pass\n",
        "    z1, a1 = forward_pass(X, W1, b1)\n",
        "\n",
        "    # Print cost every 10 iterations\n",
        "    if passes % 10 == 0:\n",
        "        cost = mse_cost(a1, y)\n",
        "        print(f'Pass {passes}, Cost: {cost}')\n",
        "\n",
        "    # Update parameters using backpropagation\n",
        "    W1, b1 = backpropagation(X, y, W1, b1, z1, a1)\n",
        "\n",
        "# Output final predictions after training\n",
        "z1, a1 = forward_pass(X, W1, b1)\n",
        "print(\"\\nFinal predictions:\")\n",
        "print(a1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZknAe2fKR37i",
        "outputId": "95495f7e-2d53-430f-eb7e-40f5c3480a6c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pass 0, Cost: 0.048018052021927264\n",
            "Pass 10, Cost: 0.035606833356200324\n",
            "Pass 20, Cost: 0.027082366250403618\n",
            "Pass 30, Cost: 0.021114523358353895\n",
            "Pass 40, Cost: 0.016850443770895333\n",
            "Pass 50, Cost: 0.013743216600767867\n",
            "Pass 60, Cost: 0.01143756228450613\n",
            "Pass 70, Cost: 0.009698444008282956\n",
            "Pass 80, Cost: 0.00836725526181136\n",
            "Pass 90, Cost: 0.007334835667043262\n",
            "\n",
            "Final predictions:\n",
            "[[0.69436042]\n",
            " [0.7859059 ]]\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOOiHWDNMl5pwo4w4oSAb0P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aherre52/MAT422/blob/main/HW_3_4_MAT_422.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **HW 3.4: Logistic regression**\n",
        "\n",
        "Concepts covered:\n",
        "\n",
        "\n",
        "* 3.4 Logistic regression"
      ],
      "metadata": {
        "id": "ix_h5JSajFrt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.4 Logistic regression\n",
        "\n",
        "Logistic regression models the probability of a binary outcome (Yes/No, True/False, Obese/Not-obese, etc), where given input data points $(\\alpha_i, b_i)$ with $\\alpha_i \\in \\mathbb{R}^d$ as feature vectors and $b_i \\in \\{0, 1\\}$ as labels, we aim to approximate the probability that $b = 1$. This model uses the **logit function** $\\log \\frac{p(\\alpha; x)}{1 - p(\\alpha; x)} = \\alpha^T x$, where $p(\\alpha; x) = \\sigma(\\alpha^T x)$, and $\\sigma$ is the sigmoid function, defined as $\\sigma(t) = \\frac{1}{1 + e^{-t}}$. To fit the model to data, we minimize the cross-entropy loss:\n",
        "\n",
        "$$\\mathcal{L}(x; A, b) = -\\frac{1}{n} \\sum_{i=1}^n \\left( b_i \\log(\\sigma(\\alpha_i^T x)) + (1 - b_i) \\log(1 - \\sigma(\\alpha_i^T x)) \\right)$$\n",
        "\n",
        "This approach benefits from gradient descent since it is convex, where we iteratively update $x$ to minimize $\\mathcal{L}(x; A, b)$. The gradient is computed as $$\\nabla_x \\mathcal{L}(x; A, b) = -\\frac{1}{n} \\sum_{i=1}^n (b_i - \\sigma(\\alpha_i^T x)) \\alpha_i$$ with each step improving the model's fit to accurately classify inputs.\n"
      ],
      "metadata": {
        "id": "leV5PiXPKv44"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code Description\n",
        "\n",
        "The code implements logistic regression to model a binary dependent variable using gradient descent optimization. It begins by generating random sample data with 100 samples and 2 features, and then models binary labels, $b \\in \\{0,1\\}$, using a binomial distribution. The cross-entropy loss function is defined to evaluate prediction error, and its gradient is calculated to update the parameters. Gradient descent iteratively adjusts parameters $x$ to minimize the cross-entropy loss, resulting in an optimal parameter vector $\\text{x}_{optimal}$. Finally, the code uses these optimized parameters to predict probabilities for new data points.\n"
      ],
      "metadata": {
        "id": "8hcwBN19JzPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# generate the sample data\n",
        "np.random.seed(42)\n",
        "n_samples = 100\n",
        "# Number of features\n",
        "d = 2\n",
        "A = np.random.randn(n_samples, d)\n",
        "\n",
        "# modeling a binary dependent variable, so using binomial distribution\n",
        "b = np.random.binomial(1, 0.5, size=n_samples)\n",
        "\n",
        "# Define the sigmoid function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Define the cross-entropy loss function\n",
        "def cross_entropy_loss(x, A, b):\n",
        "    epsilon = 1e-10  # To avoid log(0) error\n",
        "    predictions = sigmoid(A.dot(x))\n",
        "    return -np.mean(b * np.log(predictions + epsilon) + (1 - b) * np.log(1 - predictions + epsilon))\n",
        "\n",
        "# Gradient of the cross-entropy loss\n",
        "def gradient(x, A, b):\n",
        "    predictions = sigmoid(A.dot(x))\n",
        "    return -np.mean((b - predictions)[:, None] * A, axis=0)\n",
        "\n",
        "# Gradient descent optimization\n",
        "def gradient_descent(A, b, x0, learning_rate=0.1, max_iter=1000, tol=1e-5):\n",
        "    x = x0\n",
        "    for _ in range(max_iter):\n",
        "        gradient_value = gradient(x, A, b)\n",
        "        x = x - learning_rate * gradient_value\n",
        "        if np.linalg.norm(gradient_value) < tol:\n",
        "            break\n",
        "    return x\n",
        "\n",
        "# Initial guess for parameters\n",
        "x0 = np.zeros(d)\n",
        "\n",
        "# Perform gradient descent\n",
        "optimal_x = gradient_descent(A, b, x0)\n",
        "\n",
        "# Predict probabilities for new data\n",
        "new_data = np.random.randn(10, d)\n",
        "predicted_probabilities = sigmoid(new_data.dot(optimal_x))\n",
        "\n",
        "# can print out the values from gradient descent and sigmoid functions\n",
        "print(\"Optimal parameters:\", optimal_x)\n",
        "print(\"Predicted probabilities:\", predicted_probabilities)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwZbcOQ7JyNA",
        "outputId": "8475cb45-104d-4904-cbc3-61ad1d892347"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal parameters: [-0.07255775 -0.19126973]\n",
            "Predicted probabilities: [0.47464325 0.52465884 0.52732202 0.47010067 0.52663187 0.4690272\n",
            " 0.45526782 0.54172665 0.45736198 0.49476904]\n"
          ]
        }
      ]
    }
  ]
}